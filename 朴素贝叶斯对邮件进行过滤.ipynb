{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import re\n",
    "import numpy as np\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#包含所有文档 不含重复词的list\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet=set([])#创建空集，set是返回不带重复词的list\n",
    "    for document in dataSet:\n",
    "        vocabSet=vocabSet|set(document) #创建两个集合的并集\n",
    "    return list(vocabSet)\n",
    "#判断某个词条在文档中是否出现\n",
    "def setOfWords2Vec(vocabList, inputSet):#参数为词汇表和某个文档\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(\"单词: %s 不在我的词汇里面!\" % word)#返回文档向量 表示某个词是否在输入文档中出现过 1/0\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#朴素贝叶斯分类训练函数\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix)\n",
    "    numWords=len(trainMatrix[0])\n",
    "    pAbusive=sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):#遍历每个文档\n",
    "        if trainCategory[i]==1:\n",
    "            p1Num+=trainMatrix[i]\n",
    "            p1Denom+=sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num+=trainMatrix[i]\n",
    "            p0Denom+=sum(trainMatrix[i])\n",
    "\n",
    "    p1Vect = log(p1Num / p1Denom)\n",
    "    p0Vect = log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive\n",
    "\n",
    "#给定词向量 判断类别\n",
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    p1=sum(vec2Classify*p1Vec)+log(pClass1)\n",
    "    p0=sum(vec2Classify*p0Vec)+log(1.0-pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "分类错误的是：[]\n",
      "分类错误的是：[]\n",
      "分类错误的是：[]\n",
      "分类错误的是：[]\n",
      "分类错误的是：[]\n",
      "错误率是: 0.5\n"
     ]
    }
   ],
   "source": [
    "#示例：过滤垃圾邮件\n",
    "#预处理\n",
    "def textParse(bigString):\n",
    "    listOfTokens=re.split(r'x*',bigString)  #接收一个大字符串并将其解析为字符串列表\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok)>2] #去掉少于两个的字符串并全部转化为小写\n",
    "#过滤邮件 训练+测试\n",
    "def spamTest():\n",
    "    docList=[]; classList=[]; fullText=[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('./email/spam/{}.txt'.format(i),'rb').read().decode('utf-8','ignore'))\n",
    "        print(wordList)\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('./email/ham/{}.txt'.format(i),'rb').read().decode('utf-8','ignore'))\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList=createVocabList(docList)\n",
    "    trainingSet = list(range(50))\n",
    "    testSet=[]\n",
    "    for i in range(10):\n",
    "        randIndex=int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])\n",
    "    trainMat=[]; trainClasses=[]\n",
    "    for docIndex in trainingSet:\n",
    "        trainMat.append(setOfWords2Vec(vocabList,docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam=trainNB0(np.array(trainMat),np.array(trainClasses))\n",
    "    errorCount=0\n",
    "    for docIndex in testSet:\n",
    "        wordVector=setOfWords2Vec(vocabList,docList[docIndex])\n",
    "        if classifyNB(np.array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:\n",
    "            errorCount+=1\n",
    "            print(\"分类错误的是：{}\".format(docList[docIndex]))\n",
    "    print('错误率是:',float(errorCount)/len(testSet))\n",
    "\n",
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
